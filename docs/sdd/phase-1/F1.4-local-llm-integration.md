# F1.4: Local LLM Integration

**Feature ID:** F1.4
**Priority:** P0 (MVP)
**Status:** Not Started
**Package:** `@writeforge/core-llm`

---

## Overview

Integrate local LLM inference for AI-powered writing suggestions. Uses Qwen2.5 models via llama.cpp (desktop) and WebLLM (browser) to provide contextual grammar suggestions, sentence rewriting, and tone detection without any cloud dependency.

### Goals

1. Provide AI suggestions without sending data to external servers
2. Support browser (WebLLM/WebGPU) and desktop (llama.cpp) runtimes
3. Enable tone detection (formal/casual/confident/friendly)
4. Support sentence-level rewriting
5. Download and cache models efficiently

### Non-Goals

- Full document rewriting (P2)
- Custom model fine-tuning (P3)
- BYOK cloud AI integration (P3)

---

## Architecture

```
┌──────────────────────────────────────────────────────────────────┐
│                     @writeforge/core-llm                          │
├──────────────────────────────────────────────────────────────────┤
│                                                                    │
│  ┌─────────────────────────────────────────────────────────────┐  │
│  │                    LLMService (Abstract)                     │  │
│  │  - generate(prompt): Promise<string>                        │  │
│  │  - detectTone(text): Promise<Tone>                          │  │
│  │  - rewrite(text, options): Promise<string>                  │  │
│  │  - suggest(text, context): Promise<Suggestion[]>            │  │
│  └──────────────────────────┬──────────────────────────────────┘  │
│                             │                                      │
│           ┌─────────────────┴─────────────────┐                   │
│           ▼                                   ▼                   │
│  ┌─────────────────────┐          ┌─────────────────────┐        │
│  │  WebLLMProvider     │          │  LlamaCppProvider   │        │
│  │  (Browser)          │          │  (Desktop)          │        │
│  │                     │          │                     │        │
│  │  - WebGPU accel.    │          │  - Native speed     │        │
│  │  - Qwen2.5-0.5B     │          │  - Qwen2.5-1.5B+    │        │
│  │  - IndexedDB cache  │          │  - File system      │        │
│  └─────────────────────┘          └─────────────────────┘        │
│                                                                    │
│  ┌─────────────────────────────────────────────────────────────┐  │
│  │                    Model Manager                             │  │
│  │  - Download models with progress                            │  │
│  │  - Verify checksums                                         │  │
│  │  - Cache management                                         │  │
│  │  - Model config loading                                     │  │
│  └─────────────────────────────────────────────────────────────┘  │
│                                                                    │
│  ┌─────────────────────────────────────────────────────────────┐  │
│  │                    Prompt Templates                          │  │
│  │  - Grammar correction                                       │  │
│  │  - Tone detection                                           │  │
│  │  - Sentence rewriting                                       │  │
│  │  - Contextual suggestions                                   │  │
│  └─────────────────────────────────────────────────────────────┘  │
└──────────────────────────────────────────────────────────────────┘
```

---

## Model Selection

### Recommended Models

| Model | Parameters | Size (Q4) | RAM | Use Case |
|-------|------------|-----------|-----|----------|
| Qwen2.5-0.5B-Instruct | 0.5B | ~350MB | ~500MB | Browser, mobile |
| Qwen2.5-1.5B-Instruct | 1.5B | ~900MB | ~1.2GB | Desktop standard |
| Qwen2.5-3B-Instruct | 3B | ~1.8GB | ~2.4GB | Desktop power users |

### Why Qwen2.5?

1. **Apache 2.0 license** - Fully permissive for commercial use
2. **Strong instruction following** - Good at structured prompts
3. **Multilingual** - Supports 29+ languages
4. **Small model quality** - 0.5B/1.5B punch above their weight
5. **Active development** - Alibaba continues improving

### Quantization

Use Q4_K_M quantization for best quality/size tradeoff:
- 4-bit weights with mixed precision
- ~75% size reduction
- ~95% quality retention

---

## Data Models

### LLMConfig

```typescript
interface LLMConfig {
  /** Model identifier */
  modelId: string;

  /** Model file name */
  fileName: string;

  /** Download URL */
  downloadUrl: string;

  /** Expected file size in bytes */
  fileSize: number;

  /** SHA256 checksum */
  checksum: string;

  /** Context window size */
  contextLength: number;

  /** Default generation parameters */
  defaultParams: GenerationParams;
}
```

### GenerationParams

```typescript
interface GenerationParams {
  /** Maximum tokens to generate */
  maxTokens: number;

  /** Temperature for sampling (0-2) */
  temperature: number;

  /** Top-p nucleus sampling */
  topP: number;

  /** Repetition penalty */
  repetitionPenalty: number;

  /** Stop sequences */
  stopSequences: string[];
}
```

### Suggestion

```typescript
interface Suggestion {
  /** Unique identifier */
  id: string;

  /** Type of suggestion */
  type: 'grammar' | 'rewrite' | 'tone' | 'clarity';

  /** Original text segment */
  original: string;

  /** Suggested replacement */
  replacement: string;

  /** Explanation of the suggestion */
  explanation: string;

  /** Confidence score (0-1) */
  confidence: number;

  /** Offset in original text */
  offset: number;

  /** Length in original text */
  length: number;
}
```

### Tone

```typescript
type Tone = 'formal' | 'casual' | 'confident' | 'friendly' | 'neutral';

interface ToneAnalysis {
  /** Primary detected tone */
  primary: Tone;

  /** Confidence scores for each tone */
  scores: Record<Tone, number>;

  /** Segments with specific tones */
  segments: Array<{
    text: string;
    tone: Tone;
    offset: number;
  }>;
}
```

---

## API Design

### LLMService (Abstract)

```typescript
abstract class LLMService {
  protected config: LLMConfig;
  protected params: GenerationParams;

  constructor(config: LLMConfig);

  /**
   * Check if model is loaded and ready
   */
  abstract isReady(): boolean;

  /**
   * Load model into memory
   */
  abstract load(): Promise<void>;

  /**
   * Unload model from memory
   */
  abstract unload(): Promise<void>;

  /**
   * Generate text from prompt
   */
  abstract generate(prompt: string, params?: Partial<GenerationParams>): Promise<string>;

  /**
   * Detect tone of text
   */
  async detectTone(text: string): Promise<ToneAnalysis> {
    const prompt = PromptTemplates.toneDetection(text);
    const response = await this.generate(prompt, { maxTokens: 100 });
    return this.parseToneResponse(response);
  }

  /**
   * Rewrite text with options
   */
  async rewrite(text: string, options: RewriteOptions): Promise<string> {
    const prompt = PromptTemplates.rewrite(text, options);
    return this.generate(prompt, { maxTokens: text.length * 2 });
  }

  /**
   * Get contextual suggestions for text
   */
  async suggest(text: string, context: SuggestionContext): Promise<Suggestion[]> {
    const prompt = PromptTemplates.suggest(text, context);
    const response = await this.generate(prompt, { maxTokens: 500 });
    return this.parseSuggestions(response, text);
  }
}
```

### WebLLMProvider

```typescript
// src/providers/WebLLMProvider.ts

import { CreateMLCEngine, MLCEngine } from '@mlc-ai/web-llm';

export class WebLLMProvider extends LLMService {
  private engine: MLCEngine | null = null;
  private loading = false;

  async load(): Promise<void> {
    if (this.engine || this.loading) return;

    this.loading = true;

    try {
      this.engine = await CreateMLCEngine(this.config.modelId, {
        initProgressCallback: (progress) => {
          this.onLoadProgress?.(progress.progress);
        },
      });
    } finally {
      this.loading = false;
    }
  }

  async unload(): Promise<void> {
    if (this.engine) {
      await this.engine.unload();
      this.engine = null;
    }
  }

  isReady(): boolean {
    return this.engine !== null;
  }

  async generate(prompt: string, params?: Partial<GenerationParams>): Promise<string> {
    if (!this.engine) {
      throw new Error('Model not loaded');
    }

    const mergedParams = { ...this.params, ...params };

    const response = await this.engine.chat.completions.create({
      messages: [{ role: 'user', content: prompt }],
      max_tokens: mergedParams.maxTokens,
      temperature: mergedParams.temperature,
      top_p: mergedParams.topP,
      stop: mergedParams.stopSequences,
    });

    return response.choices[0]?.message?.content || '';
  }

  /**
   * Callback for load progress
   */
  onLoadProgress?: (progress: number) => void;
}
```

### LlamaCppProvider

```typescript
// src/providers/LlamaCppProvider.ts

// Note: This would use llama.cpp bindings via NAPI or FFI
// Simplified for documentation

export class LlamaCppProvider extends LLMService {
  private context: LlamaContext | null = null;

  async load(): Promise<void> {
    const modelPath = await this.modelManager.getModelPath(this.config.modelId);

    // Initialize llama.cpp context
    this.context = await LlamaContext.create({
      modelPath,
      contextSize: this.config.contextLength,
      gpuLayers: -1, // Use all GPU layers if available
    });
  }

  async unload(): Promise<void> {
    if (this.context) {
      this.context.dispose();
      this.context = null;
    }
  }

  isReady(): boolean {
    return this.context !== null;
  }

  async generate(prompt: string, params?: Partial<GenerationParams>): Promise<string> {
    if (!this.context) {
      throw new Error('Model not loaded');
    }

    const mergedParams = { ...this.params, ...params };

    const tokens = await this.context.tokenize(prompt);

    let output = '';
    for await (const token of this.context.generate(tokens, {
      maxTokens: mergedParams.maxTokens,
      temperature: mergedParams.temperature,
      topP: mergedParams.topP,
      repeatPenalty: mergedParams.repetitionPenalty,
      stopStrings: mergedParams.stopSequences,
    })) {
      output += token;
    }

    return output.trim();
  }
}
```

### Model Manager

```typescript
// src/ModelManager.ts

export class ModelManager {
  private storageProvider: StorageProvider;
  private configs: Map<string, LLMConfig>;

  constructor(storageProvider: StorageProvider) {
    this.storageProvider = storageProvider;
    this.configs = new Map();
    this.loadConfigs();
  }

  /**
   * Check if model is downloaded
   */
  async isDownloaded(modelId: string): Promise<boolean> {
    const config = this.configs.get(modelId);
    if (!config) return false;

    return this.storageProvider.exists(config.fileName);
  }

  /**
   * Download model with progress reporting
   */
  async download(
    modelId: string,
    onProgress?: (downloaded: number, total: number) => void
  ): Promise<void> {
    const config = this.configs.get(modelId);
    if (!config) {
      throw new Error(`Unknown model: ${modelId}`);
    }

    const response = await fetch(config.downloadUrl);
    const reader = response.body!.getReader();
    const contentLength = config.fileSize;

    const chunks: Uint8Array[] = [];
    let downloaded = 0;

    while (true) {
      const { done, value } = await reader.read();
      if (done) break;

      chunks.push(value);
      downloaded += value.length;
      onProgress?.(downloaded, contentLength);
    }

    const blob = new Blob(chunks);
    const arrayBuffer = await blob.arrayBuffer();

    // Verify checksum
    const hash = await this.sha256(arrayBuffer);
    if (hash !== config.checksum) {
      throw new Error('Checksum mismatch - download corrupted');
    }

    await this.storageProvider.save(config.fileName, arrayBuffer);
  }

  /**
   * Delete downloaded model
   */
  async delete(modelId: string): Promise<void> {
    const config = this.configs.get(modelId);
    if (!config) return;

    await this.storageProvider.delete(config.fileName);
  }

  /**
   * Get model file path (desktop) or blob URL (browser)
   */
  async getModelPath(modelId: string): Promise<string> {
    const config = this.configs.get(modelId);
    if (!config) {
      throw new Error(`Unknown model: ${modelId}`);
    }

    return this.storageProvider.getPath(config.fileName);
  }

  private async sha256(buffer: ArrayBuffer): Promise<string> {
    const hashBuffer = await crypto.subtle.digest('SHA-256', buffer);
    const hashArray = Array.from(new Uint8Array(hashBuffer));
    return hashArray.map(b => b.toString(16).padStart(2, '0')).join('');
  }
}
```

---

## Prompt Templates

```typescript
// src/prompts/templates.ts

export const PromptTemplates = {
  /**
   * Tone detection prompt
   */
  toneDetection: (text: string) => `
Analyze the tone of the following text. Respond with a JSON object containing:
- primary: the main tone (one of: formal, casual, confident, friendly, neutral)
- scores: confidence scores (0-1) for each tone

Text: "${text}"

JSON:`,

  /**
   * Grammar suggestion prompt
   */
  grammarSuggestion: (text: string) => `
Check the following text for grammar errors. For each error found, provide:
- original: the problematic text
- correction: the suggested fix
- explanation: brief explanation of the error

If no errors, respond with "No errors found."

Text: "${text}"

Analysis:`,

  /**
   * Rewrite prompt
   */
  rewrite: (text: string, options: RewriteOptions) => {
    const toneInstructions = options.targetTone
      ? `Make the tone ${options.targetTone}.`
      : '';

    const lengthInstructions = options.shorter
      ? 'Make it more concise.'
      : options.longer
        ? 'Expand with more detail.'
        : '';

    return `
Rewrite the following text. ${toneInstructions} ${lengthInstructions}
Only output the rewritten text, nothing else.

Original: "${text}"

Rewritten:`;
  },

  /**
   * Contextual suggestion prompt
   */
  suggest: (text: string, context: SuggestionContext) => `
Given the following text, provide suggestions for improvement.
Context: ${context.type}
${context.previousSentence ? `Previous sentence: "${context.previousSentence}"` : ''}
${context.nextSentence ? `Next sentence: "${context.nextSentence}"` : ''}

Text: "${text}"

Provide up to 3 suggestions in JSON format:
[{"type": "...", "original": "...", "replacement": "...", "explanation": "..."}]

Suggestions:`,
};
```

---

## File Structure

```
packages/core/llm/
├── src/
│   ├── index.ts                  # Public exports
│   ├── LLMService.ts             # Abstract service class
│   ├── ModelManager.ts           # Model download/cache
│   ├── providers/
│   │   ├── WebLLMProvider.ts     # Browser implementation
│   │   └── LlamaCppProvider.ts   # Desktop implementation
│   ├── prompts/
│   │   ├── templates.ts          # Prompt templates
│   │   └── parsing.ts            # Response parsers
│   ├── storage/
│   │   ├── StorageProvider.ts    # Abstract storage
│   │   ├── IndexedDBStorage.ts   # Browser storage
│   │   └── FileSystemStorage.ts  # Desktop storage
│   ├── types.ts                  # TypeScript interfaces
│   └── __tests__/
│       ├── LLMService.test.ts
│       ├── ModelManager.test.ts
│       └── prompts.test.ts
├── models/
│   └── configs.json              # Model configurations
├── package.json
├── tsconfig.json
└── README.md
```

---

## Model Configurations

```json
{
  "models": [
    {
      "modelId": "Qwen2.5-0.5B-Instruct-q4f16_1-MLC",
      "fileName": "qwen2.5-0.5b-instruct-q4f16_1.wasm",
      "downloadUrl": "https://huggingface.co/mlc-ai/Qwen2.5-0.5B-Instruct-q4f16_1-MLC/resolve/main/",
      "fileSize": 367001600,
      "checksum": "abc123...",
      "contextLength": 32768,
      "runtime": "webllm",
      "defaultParams": {
        "maxTokens": 256,
        "temperature": 0.7,
        "topP": 0.9,
        "repetitionPenalty": 1.1,
        "stopSequences": ["\n\n", "Human:", "User:"]
      }
    },
    {
      "modelId": "qwen2.5-1.5b-instruct-q4_k_m",
      "fileName": "qwen2.5-1.5b-instruct-q4_k_m.gguf",
      "downloadUrl": "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q4_k_m.gguf",
      "fileSize": 986710016,
      "checksum": "def456...",
      "contextLength": 32768,
      "runtime": "llama.cpp",
      "defaultParams": {
        "maxTokens": 512,
        "temperature": 0.7,
        "topP": 0.9,
        "repetitionPenalty": 1.1,
        "stopSequences": ["\n\n", "Human:", "User:"]
      }
    }
  ]
}
```

---

## Performance Targets

| Metric | Browser (0.5B) | Desktop (1.5B) |
|--------|----------------|----------------|
| Model load time | <10s (cached) | <5s |
| First token latency | <500ms | <200ms |
| Token generation | 15-30 tok/s | 30-60 tok/s |
| Memory usage | <800MB | <1.5GB |
| Suggestion latency | <2s | <1s |

---

## Error Handling

```typescript
export class LLMError extends Error {
  constructor(
    message: string,
    public readonly code: LLMErrorCode,
    public readonly cause?: Error
  ) {
    super(message);
    this.name = 'LLMError';
  }
}

export enum LLMErrorCode {
  MODEL_NOT_FOUND = 'MODEL_NOT_FOUND',
  MODEL_NOT_LOADED = 'MODEL_NOT_LOADED',
  DOWNLOAD_FAILED = 'DOWNLOAD_FAILED',
  CHECKSUM_MISMATCH = 'CHECKSUM_MISMATCH',
  GENERATION_FAILED = 'GENERATION_FAILED',
  OUT_OF_MEMORY = 'OUT_OF_MEMORY',
  WEBGPU_NOT_SUPPORTED = 'WEBGPU_NOT_SUPPORTED',
}
```

---

## WebGPU Fallback

For browsers without WebGPU support:

```typescript
async function createLLMProvider(): Promise<LLMService> {
  // Check WebGPU support
  if ('gpu' in navigator) {
    const adapter = await navigator.gpu.requestAdapter();
    if (adapter) {
      return new WebLLMProvider(browserConfig);
    }
  }

  // Fallback: Disable AI features, use grammar-only mode
  logger.warn('WebGPU not available - AI features disabled');
  return new NoOpLLMProvider();
}
```

---

## Testing Strategy

### Unit Tests

```typescript
describe('LLMService', () => {
  describe('detectTone', () => {
    it('should detect formal tone', async () => {
      const service = new MockLLMProvider();
      const result = await service.detectTone(
        'I am writing to inform you of the upcoming meeting.'
      );

      expect(result.primary).toBe('formal');
      expect(result.scores.formal).toBeGreaterThan(0.7);
    });

    it('should detect casual tone', async () => {
      const service = new MockLLMProvider();
      const result = await service.detectTone(
        "Hey! What's up? Wanna grab lunch?"
      );

      expect(result.primary).toBe('casual');
    });
  });

  describe('rewrite', () => {
    it('should make text more formal', async () => {
      const service = new MockLLMProvider();
      const result = await service.rewrite("Hey, what's up?", {
        targetTone: 'formal',
      });

      expect(result).not.toContain('Hey');
      expect(result.toLowerCase()).toContain('hello');
    });
  });
});
```

### Integration Tests

```typescript
describe('WebLLM Integration', () => {
  // These tests require a browser environment with WebGPU
  // Run via Playwright

  it('should load model successfully', async () => {
    const provider = new WebLLMProvider(testConfig);
    await provider.load();

    expect(provider.isReady()).toBe(true);
  });

  it('should generate coherent text', async () => {
    const provider = new WebLLMProvider(testConfig);
    await provider.load();

    const result = await provider.generate('The quick brown');

    expect(result.length).toBeGreaterThan(0);
    expect(result).toMatch(/fox|dog|cat/i); // Common completions
  });
});
```

---

## Acceptance Criteria

- [ ] WebLLM provider works in Chrome 113+ with WebGPU
- [ ] llama.cpp provider works on macOS (Apple Silicon + Intel)
- [ ] Model download shows progress and can be cancelled
- [ ] Downloaded models verified via checksum
- [ ] Tone detection returns valid analysis
- [ ] Rewriting produces coherent alternative text
- [ ] Suggestions are contextually relevant
- [ ] Fallback to grammar-only mode when LLM unavailable
- [ ] Memory usage within targets
- [ ] 85% test coverage

---

**Document Version:** 1.0.0
**Created:** January 2026
**Author:** AI Assistant
